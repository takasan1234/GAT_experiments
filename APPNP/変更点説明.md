# APPNP実装での変更点説明

## 概要
元のGAT実装をAPPNP（Approximate Personalized Propagation of Neural Predictions）に変更しました。

## 主な変更点

### 1. ライブラリの変更
- PyTorch Geometricのインポートを削除し、独自のAPPNP実装を使用
- 既存のscipy.sparseやその他の基本ライブラリはそのまま維持

### 2. モデルアーキテクチャの変更
**変更前（GAT）:**
```python
class GAT(nn.Module):
    # マルチヘッドアテンション機構
    self.attentions = [GraphAttentionLayer(...) for _ in range(nheads)]
    self.out_att = GraphAttentionLayer(...)
```

**変更後（APPNP）:**
```python
class APPNPNet(nn.Module):
    # 2層MLP + APPNP伝播
    self.mlp = nn.Sequential(
        nn.Linear(num_features, hidden_dim),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(hidden_dim, num_classes)
    )
    # APPNPパラメータ（K, alpha, dropout）
```

### 3. ハイパーパラメータの変更
**指定された条件に従って以下のパラメータを設定:**
- `alpha = 0.1` (テレポート確率)
- `K = 10` (伝播ステップ数)
- `hidden_dim = 64` (隠れ層次元数)
- `dropout = 0.5` (ドロップアウト率)

**最適化手法の変更:**
- 学習率: `0.0001` → `0.01`
- 重み減衰: `0.0001` → `5e-4`

### 4. APPNP伝播の実装
元のGATのアテンション機構を、APPNPの伝播ステップに置き換え:
```python
def forward(self, x, adj):
    h0 = self.mlp(x)  # 初期予測
    h = h0.clone()
    
    # K回の伝播
    for k in range(self.K):
        if self.training:
            h = F.dropout(h, p=self.dropout, training=self.training)
        h = (1 - self.alpha) * torch.mm(adj, h) + self.alpha * h0
    
    return h
```

### 5. Early Stoppingの改善
- `patience = 0.001` → `patience = 100` (より実用的な値に変更)
- 最大エポック数: `300` → `1000`
- 検証時の予測を適切に分離

### 6. データローディングの維持
**変更なし（データリーク防止のため）:**
- `load_data()` 関数はそのまま維持
- 既存のtrain/validation/test分割方法も維持
- `accuracy()` 関数も既存のものを使用

### 7. 評価とテスト部分の維持
- テスト精度の評価方法は変更なし
- 可視化（t-SNE）機能も維持

## APPNPアルゴリズムの特徴
1. **初期特徴抽出**: 2層MLPで初期予測を生成
2. **パーソナライズPageRank**: グラフ構造を利用した伝播処理
3. **テレポート機構**: 初期予測と近傍情報のバランス調整

## 期待される効果
- GATよりもシンプルな実装
- 効率的な伝播処理
- 過平滑化の軽減（αパラメータによる制御）
- 安定した学習性能
