# GCNII実装での変更点説明

## 概要
元のGAT実装をGCNII（Simple and Deep Graph Convolutional Networks）に変更しました。

## 主な変更点

### 1. 論文情報の更新
**変更前（GAT）:**
- 論文: "Graph Attention Networks" (ICLR 2018)
- GitHub: https://github.com/Diego999/pyGAT

**変更後（GCNII）:**
- 論文: "Simple and Deep Graph Convolutional Networks" (ICML 2020)
- GitHub: https://github.com/chennnM/GCNII

### 2. モデルアーキテクチャの完全変更

**変更前（GAT）:**
```python
class GraphAttentionLayer(nn.Module):
    # マルチヘッドアテンション機構
    def forward(self, h, adj):
        attention = F.softmax(e, dim=1)
        h_prime = torch.matmul(attention, Wh)
```

**変更後（GCNII）:**
```python
class GCNIILayer(nn.Module):
    # Identity mappingとInitial residual connection
    def forward(self, input, adj, h0):
        # GCNII formula: (1-α)P H^(l) W^(l) + α H^(0) W^(l)
        output = (1 - self.alpha) * output + self.alpha * initial_residual
        # Identity mapping (skip connection)
        output = output + self.beta * input
```

### 3. ハイパーパラメータの変更
- `nlayers = 32` (層数 - 深いネットワーク)
- `hidden_dim = 64` (隠れ層次元数)
- `alpha = 0.1` (初期残差パラメータ)
- `lamda = 0.5` (スキップ接続パラメータ λ)
- `dropout = 0.6` (ドロップアウト率)

### 4. GCNII特有の機能実装

#### 4.1 Initial Residual Connection
```python
# 初期特徴量との残差接続
initial_residual = torch.mm(h0, self.weight)
output = (1 - self.alpha) * output + self.alpha * initial_residual
```

#### 4.2 Identity Mapping (Skip Connection)
```python
# レイヤー間でのスキップ接続
self.beta = np.log(lamda / layer_idx + 1)  # 層依存の重み
if self.in_features == self.out_features:
    output = output + self.beta * input
```

#### 4.3 深い層数への対応
- 32層の深いネットワーク構造
- 各層での勾配消失問題の回避
- 過平滑化問題の解決

### 5. 最適化手法の変更
**GCNII特有の最適化:**
```python
# パラメータを分離した最適化
optimizer = torch.optim.Adam([
    {'params': model.params1, 'weight_decay': 5e-4},  # GCNII layers
    {'params': model.params2, 'weight_decay': 5e-4}   # Linear layers
], lr=0.01)
```

**変更前:**
- 学習率: `0.0001`
- 重み減衰: `0.0001`
- 統一的なパラメータ最適化

**変更後:**
- 学習率: `0.01`
- 重み減衰: `5e-4`
- パラメータ分離最適化

### 6. 損失関数の変更
- `nn.CrossEntropyLoss()` → `nn.NLLLoss()`
- `F.log_softmax()` の使用に対応

### 7. 学習設定の深いモデル対応
**深いモデル用の調整:**
- 最大エポック数: `300` → `1500`
- 出力頻度: 10エポック毎 → 50エポック毎
- Early stopping patience: `0.001` → `200`

### 8. データローディングの維持
**変更なし（データリーク防止のため）:**
- `load_data()` 関数はそのまま維持
- 既存のtrain/validation/test分割方法も維持
- `accuracy()` 関数も既存のものを使用

### 9. 可視化の更新
- t-SNE可視化のタイトルをGCNII用に更新
- グラフのラベルと凡例を改善
- 損失関数名を"Negative Log Likelihood Loss"に変更

## GCNIIアルゴリズムの特徴

### 1. 深いネットワークの実現
- 32層という深い構造でも安定した学習
- 勾配消失問題の効果的な回避

### 2. Initial Residual Connection
- 初期特徴量との直接的な接続
- 各層で初期情報を保持

### 3. Identity Mapping
- 層間でのスキップ接続
- 過平滑化問題の軽減

### 4. Layer-wise Adaptive Weights
- 各層で異なる重み係数β
- 深さに応じた適応的な調整

## 期待される効果
- **深いネットワーク**: 32層でもオーバーフィッティングを回避
- **表現力向上**: 複雑なグラフパターンの学習
- **安定性**: Skip connectionによる安定した勾配流
- **汎化性能**: Initial residual connectionによる情報保持
- **計算効率**: シンプルな線形変換ベースの設計

## [GCNII論文](https://github.com/chennnM/GCNII)での報告性能
- Cora: 85.5% (64層)
- Citeseer: 73.4% (32層)  
- Pubmed: 80.3% (16層)
