# 工夫1: GATパラメータ調整実験

## 概要
初期のGAT実装をベースに、以下の4つの観点から改良・実験を行いました：
1. Multi-head Attention数の調整
2. Dropout率の調整  
3. バイアス項の有無の検証
4. Early Stoppingの改良実装

## 変更点詳細

### 1. Multi-head Attention数の実験
**目的**: Attention head数が性能に与える影響を調査

**変更内容**:
- 初期設定: 8 heads → 4, 8, 16 headsでの比較実験
- 各設定での学習曲線と最終精度を比較

**期待効果**:
- 少ないhead数: 計算効率向上、過学習抑制
- 多いhead数: より複雑なパターン学習、表現力向上

### 2. Dropout率の調整実験
**目的**: 正則化の強さが性能に与える影響を調査

**変更内容**:
- 初期設定: 0.2 → 0.0, 0.3, 0.6での比較実験
- 訓練・検証・テスト精度の比較による過学習の分析

**期待効果**:
- Dropout 0.0: 最大表現力、過学習リスク
- Dropout 0.3: 適度な正則化
- Dropout 0.6: 強い正則化、underfittingリスク

### 3. バイアス項の有無実験
**目的**: バイアス項が学習効率と精度に与える影響を調査

**変更内容**:
```python
# 初期実装：バイアス項なし
class GraphAttentionLayer(nn.Module):
    def __init__(self, ...):
        self.W = nn.Parameter(...)
        # バイアス項なし

# 工夫1実装：バイアス項の制御
class GraphAttentionLayer(nn.Module):
    def __init__(self, ..., add_bias=True):
        self.W = nn.Parameter(...)
        if self.add_bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)
```

**期待効果**:
- バイアス有り: より柔軟な学習、収束改善
- バイアス無し: パラメータ削減、シンプルな表現

### 4. Early Stoppingの改良実装
**目的**: 過学習防止と学習効率の向上

**変更内容**:
```python
# 初期実装：シンプルなpatience counter
if use_early_stopping:
    if val_loss < last_min_val_loss:
        last_min_val_loss = val_loss
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter == patience:  # patience=0.001 (非常に小さく、実質無効)
            stopped_early = True

# 工夫1実装：専用クラスによる改良
class EarlyStopping:
    def __init__(self, patience=10, min_delta=1e-4, restore_best_weights=True):
        self.patience = patience          # より適切なpatience値
        self.min_delta = min_delta        # 最小改善量の閾値
        self.restore_best_weights = True  # 最良重みの復元
        
    def __call__(self, val_loss, model):
        # 最良重みの保存・復元機能付き
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = copy.deepcopy(model.state_dict())
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False
```

**改良点**:
- より適切なpatience値（20エポック）
- 最小改善量の閾値設定（1e-4）
- 最良重みの自動保存・復元
- より安定したearly stopping判定

## 実験設定

### 比較実験一覧
1. **基本設定**: nheads=8, dropout=0.2, add_bias=True
2. **Multi-head少なめ**: nheads=4, dropout=0.2, add_bias=True
3. **Multi-head多め**: nheads=16, dropout=0.2, add_bias=True
4. **Dropout無し**: nheads=8, dropout=0.0, add_bias=True
5. **Dropout低め**: nheads=8, dropout=0.3, add_bias=True
6. **Dropout高め**: nheads=8, dropout=0.6, add_bias=True
7. **バイアス無し**: nheads=8, dropout=0.2, add_bias=False

### 評価指標
- **テスト精度**: 最終的な分類性能
- **学習時間**: 収束までの時間
- **収束エポック数**: Early stoppingによる早期終了の効果
- **学習曲線**: 訓練・検証の損失・精度推移
- **t-SNE可視化**: 学習された特徴表現の質

## 期待される結果

### Multi-head数の影響
- 4 heads: 軽量だが表現力不足の可能性
- 8 heads: バランスの取れた性能
- 16 heads: 高い表現力だが過学習リスク

### Dropout率の影響
- 高Dropout率: 汎化性能向上だが訓練精度低下
- 低Dropout率: 訓練精度高いが過学習リスク

### バイアス項の影響
- バイアス有り: より柔軟な学習
- バイアス無し: よりシンプルで汎化しやすい可能性

### Early Stoppingの効果
- 全ての実験で過学習の抑制
- 学習時間の短縮
- より安定した最終性能

## コード変更のポイント

1. **再現性確保**: 全実験で同一の乱数シード（42）
2. **データ分割固定**: 初期実装と完全に同一の分割
3. **並列実験**: 複数設定を一度に実行して効率化
4. **結果保存**: 全実験結果を辞書で管理
5. **可視化強化**: 学習曲線の比較、結果の一覧表示

## 実装上の工夫

- **設定管理**: 実験設定を辞書のリストで管理し、ループで実行
- **結果比較**: 全実験結果をまとめて可視化
- **ベストモデル特定**: 最高精度のモデルを自動特定
- **考察自動化**: 実験結果に基づく考察を自動生成 