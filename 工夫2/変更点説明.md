# 工夫2: GAT から GCNConv への変更実験

## 概要
初期のGAT実装から、PyTorch GeometricのGCNConvを使用した実装に変更し、性能比較を行います。

## 主な変更点

### 1. モデルアーキテクチャの完全変更
**目的**: GATのAttention機構とGCNの畳み込み機構の性能比較

**変更内容**:
- GAT（Graph Attention Networks）→ GCNConv（Graph Convolutional Networks）
- Multi-head attentionの代わりに、シンプルなグラフ畳み込み層を使用

### 2. 2層GCN構成の実装
**モデル構造**:
```
入力（1433次元）
↓
GCNConv(1433 → 64)
↓  
ReLU活性化
↓
BatchNorm1d(64)
↓
Dropout(0.5)
↓
GCNConv(64 → 7)
↓
出力（7クラス）
```

**設計思想**:
- **中間次元64**: GAT（隠れ32×8heads=256）より少ない次元で効率化
- **2層構成**: 表現学習に十分な深さを確保
- **BatchNorm**: 学習安定化と収束改善
- **ReLU**: シンプルで効果的な活性化関数

### 3. PyTorch Geometric仕様への適応
**変更内容**:
```python
# GAT仕様（Dense adjacency matrix）
def forward(self, x, adj):
    # adjは密行列 (N×N)
    
# GCNConv仕様（Edge index format）  
def forward(self, x, edge_index):
    # edge_indexは疎行列表現 (2×E)
```

**データ形式変換**:
- 密行列の隣接行列 → エッジインデックス形式
- メモリ効率とPyTorch Geometric標準形式への対応

### 4. cached=Trueによる最適化
**目的**: 学習効率の向上

**実装**:
```python
self.conv1 = GCNConv(in_channels, hidden_channels, cached=True)
self.conv2 = GCNConv(hidden_channels, out_channels, cached=True)
```

**効果**:
- 正規化済み隣接行列のキャッシュ
- 繰り返し計算の削減
- 学習速度の向上

### 5. Dropout率の調整
**変更内容**:
- GAT: 0.2 → GCN: 0.5
- より強い正則化でGCNの過学習を抑制

## 技術的詳細

### GCNConvレイヤーの仕様
```python
class GCNConv(MessagePassing):
    def forward(self, x, edge_index, edge_weight=None):
        # Kipf & Welling (2016)の実装
        # A_norm = D^(-1/2) * A * D^(-1/2)
        return self.propagate(edge_index, x=x, edge_weight=edge_weight)
```

### BatchNormalization追加の理由
1. **勾配消失問題の軽減**: 深いネットワークでの学習安定化
2. **内部共変量シフト対策**: 各層の入力分布を正規化  
3. **学習率調整の柔軟性**: より高い学習率での学習が可能

### データ前処理の変更
```python
# GAT用（初期実装）
adj = torch.FloatTensor(np.array(adj.todense()))  # 密行列

# GCN用（工夫2）
edge_index = torch.LongTensor(edges.T)  # エッジインデックス形式
```

## 期待される効果

### 1. 計算効率の改善
- **パラメータ数削減**: GCNはGATより少ないパラメータ
- **メモリ使用量削減**: 疎行列表現によるメモリ効率化
- **学習速度向上**: cached=Trueによる最適化

### 2. 学習安定性の向上
- **BatchNorm効果**: より安定した学習曲線
- **適切なDropout**: 過学習の抑制

### 3. 性能面での比較検証
- **表現学習能力**: AttentionなしでGATと同等以上の性能を目指す
- **汎化性能**: よりシンプルなモデルによる汎化能力
- **収束速度**: BatchNormによる高速収束

## 実験設定

### モデルパラメータ
- **入力次元**: 1433（変更なし）
- **中間次元**: 64
- **出力次元**: 7（変更なし）
- **Dropout率**: 0.5
- **学習率**: 0.0001（GAT実装と同じ）
- **重み減衰**: 0.0001（GAT実装と同じ）

### 比較項目
1. **テスト精度**: 最終的な分類性能
2. **学習速度**: エポックあたりの学習時間
3. **収束速度**: 目標精度への到達エポック数
4. **メモリ使用量**: 学習時のGPUメモリ使用量
5. **パラメータ数**: モデルの複雑性比較

## コード変更のハイライト

### 1. ライブラリ追加
```python
import torch_geometric
from torch_geometric.nn import GCNConv
```

### 2. モデルクラスの完全書き換え
```python
class GCN(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True)
        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True)
        self.bn1 = nn.BatchNorm1d(hidden_channels)
        self.dropout = dropout
        
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.bn1(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)
```

### 3. データ形式の変更
```python
# 隣接行列からエッジインデックスへの変換
edge_index = torch.LongTensor(edges.T)
```

## 実装上の注意点

1. **PyTorch Geometric依存**: torch-geometricのインストールが必要
2. **CUDA互換性**: PyTorch GeometricのCUDA対応バージョンを使用
3. **メモリ効率**: エッジインデックス形式による疎行列の活用
4. **キャッシュ機能**: cached=Trueによる計算効率化

この実装により、GATとGCNの直接的な性能比較が可能になり、グラフニューラルネットワークにおける異なるアプローチの特性を理解できます。 